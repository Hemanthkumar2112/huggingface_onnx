{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOBDmdBLASLkc4p7wj/zRsq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hemanthkumar2112/huggingface_onnx/blob/main/huggingface_onnx_quantized_model_for_sequence_classification_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBc8i6jx3a7n"
      },
      "outputs": [],
      "source": [
        "!python -m pip install git+https://github.com/huggingface/optimum.git#egg=optimum[onnxruntime]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "save_directory = \"tmp/onnx/\"\n",
        "\n",
        "# Load a model from transformers and export it to ONNX\n",
        "ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, from_transformers=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Save the onnx model and tokenizer\n",
        "ort_model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3vnzDUj33QR",
        "outputId": "0b27c4e7-15b0-48d9-a0b7-ebc559901f02"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tmp/onnx/tokenizer_config.json',\n",
              " 'tmp/onnx/special_tokens_map.json',\n",
              " 'tmp/onnx/vocab.txt',\n",
              " 'tmp/onnx/added_tokens.json',\n",
              " 'tmp/onnx/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
        "from optimum.onnxruntime import ORTQuantizer\n",
        "\n",
        "# Define the quantization methodology\n",
        "qconfig = AutoQuantizationConfig.arm64(is_static=False, per_channel=False)\n",
        "quantizer = ORTQuantizer.from_pretrained(ort_model)\n",
        "\n",
        "# Apply dynamic quantization on the model\n",
        "quantizer.quantize(save_dir=save_directory, quantization_config=qconfig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlu5MziT4KY4",
        "outputId": "04627711-a05a-4697-93ad-3260768faa30"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('tmp/onnx')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "model = ORTModelForSequenceClassification.from_pretrained(save_directory, file_name=\"model_quantized.onnx\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "\n",
        "cls_pipeline = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
        "\n"
      ],
      "metadata": {
        "id": "R9dvZxMt4qBm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "cls_pipeline(\"I love burritos!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r7f8BPW5VH6",
        "outputId": "02f1fdd6-97c2-42c4-8279-2828260b81b1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 20.1 ms, sys: 0 ns, total: 20.1 ms\n",
            "Wall time: 22.8 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9996811151504517}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8pJMHRSh5cbZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}